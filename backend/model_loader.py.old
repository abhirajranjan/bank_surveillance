import torch
import torch.nn as nn
import torchvision.models.video as video_models
from huggingface_hub import hf_hub_download
import numpy as np
import cv2
from collections import deque

# Define labels globally or pass them around
LABELS = ["arrest", "Explosion", "Fight", "normal", "roadaccidents", "shooting", "Stealing", "vandalism"]
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class I3DClassifier(nn.Module):
    def __init__(self, num_classes):
        super(I3DClassifier, self).__init__()
        # Use the new way to load pretrained models if available, or fallback
        try:
            self.i3d = video_models.i3d_r50(weights=video_models.I3D_R50_Weights.KINETICS400_V1)
        except AttributeError:
            self.i3d = video_models.i3d_r50(pretrained=True) # Fallback for older torchvision
        
        self.dropout = nn.Dropout(0.3)
        # The original I3D model has a head with 400 output features for Kinetics-400
        # We need to replace its classifier head (self.i3d.blocks[6].proj)
        # If the model structure differs slightly in your torchvision version, you might need to inspect self.i3d
        # and find the correct layer to replace. Typically it's the last linear layer.
        num_ftrs = self.i3d.blocks[6].proj.in_features # Get in_features of the existing proj layer
        self.i3d.blocks[6].proj = nn.Linear(num_ftrs, num_classes)


    def forward(self, x):
        x = self.i3d(x)
        # The output of i3d_r50 is already (batch_size, num_classes) after passing through the modified proj layer
        # No need for an extra dropout if it's already part of the model or handled by the proj layer.
        # However, if you want an additional dropout:
        # x = self.dropout(x)
        return x

def load_model(repo_id="Ahmeddawood0001/i3d_ucf_finetuned", filename="i3d_ucf_finetuned.pth"):
    print(f"Loading model on {DEVICE}...")
    model = I3DClassifier(num_classes=len(LABELS)).to(DEVICE)
    
    try:
        weights_path = hf_hub_download(repo_id=repo_id, filename=filename)
        model.load_state_dict(torch.load(weights_path, map_location=DEVICE))
        print("Model weights loaded successfully.")
    except Exception as e:
        print(f"Error loading weights: {e}")
        print("Proceeding with the base I3D model (Kinetics-400 weights, adapted head).")
        # If finetuned weights fail, the model will use Kinetics weights with a randomly initialized classifier head for 'num_classes'.
        # This won't give good results for the specific 8 classes but allows the app to run.

    model.eval()
    return model

MODEL = load_model() # Load model once when the module is imported

def preprocess_frame(frame):
    # Resize and normalize frame (example: 224x224)
    # I3D typically expects input size of 224x224 or similar
    frame = cv2.resize(frame, (224, 224))
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # BGR to RGB
    return frame # Return as uint8, normalization will happen in predict_realtime

def predict_from_buffer(frame_buffer):
    if not frame_buffer:
        return None, None

    # Create a copy of the buffer to avoid modification during processing
    frames_np = np.array(list(frame_buffer)) # Shape: (T, H, W, C)
    
    # Normalize and permute
    frames_tensor = torch.from_numpy(frames_np).float() / 255.0
    frames_tensor = frames_tensor.permute(3, 0, 1, 2)  # (C, T, H, W)
    frames_tensor = frames_tensor.unsqueeze(0).to(DEVICE)  # Add batch dimension (B, C, T, H, W)
    
    with torch.no_grad():
        output = MODEL(frames_tensor)
        probs = torch.softmax(output, dim=1)
        confidence, pred_idx = torch.max(probs, dim=1)
        return LABELS[pred_idx.item()], confidence.item()